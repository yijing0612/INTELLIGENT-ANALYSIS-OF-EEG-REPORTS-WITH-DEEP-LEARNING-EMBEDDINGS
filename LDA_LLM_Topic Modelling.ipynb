{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT CLEANING\n",
    "TEXT_CLEANING = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z]+\"\n",
    "\n",
    "def preprocess(text, stop_words, stem=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by removing unwanted characters, tokenizing, \n",
    "    removing stopwords, and optionally stemming.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to preprocess.\n",
    "        stop_words (set): A set of stopwords to remove.\n",
    "        stem (bool): Whether to apply stemming.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed words (tokens).\n",
    "    \"\"\"\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "    # Initialize stemmer if stemming is needed\n",
    "    stemmer = PorterStemmer() if stem else None\n",
    "\n",
    "    # Clean and tokenize text\n",
    "    text = re.sub(TEXT_CLEANING, ' ', str(text).lower()).strip()\n",
    "    tokens = [stemmer.stem(token) if stemmer else token for token in text.split() if token not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_lists_from_corpus(file, num_topics, words_per_topic):\n",
    "    \"\"\"\n",
    "    Extracts topics and their associated words from a PDF document using the \n",
    "    Latent Dirichlet Allocation (LDA) algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): The path to the PDF file for topic extraction.\n",
    "        num_topics (int): The number of topics to discover.\n",
    "        words_per_topic (int): The number of words to include per topic.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of num_topics sublists, each containing relevant words \n",
    "        for a topic.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file, on_bad_lines='skip', encoding='latin1')\n",
    "\n",
    "    # Check if 'text' column exists\n",
    "    if 'conclusion' not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain a 'text' column\")\n",
    "\n",
    "    # Extract the text into a list. Each row is considered a document\n",
    "    documents = df['conclusion'].tolist()\n",
    "\n",
    "    # Preprocess the documents\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words(['english','spanish']))\n",
    "    processed_documents = [preprocess(doc, stop_words) for doc in documents]\n",
    "\n",
    "    # Create a dictionary and a corpus\n",
    "    dictionary = corpora.Dictionary(processed_documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "\n",
    "    # Build the LDA model\n",
    "    lda_model = LdaModel(\n",
    "        corpus, \n",
    "        num_topics=num_topics, \n",
    "        id2word=dictionary, \n",
    "        passes=15\n",
    "        )\n",
    "\n",
    "    # Retrieve the topics and their corresponding words\n",
    "    topics = lda_model.print_topics(num_words=words_per_topic)\n",
    "\n",
    "    # Store each list of words from each topic into a list\n",
    "    topics_ls = []\n",
    "    for topic in topics:\n",
    "        words = topic[1].split(\"+\")\n",
    "        topic_words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
    "        topics_ls.append(topic_words)\n",
    "\n",
    "    return topics_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_lists_from_corpus(file, num_topics, words_per_topic):\n",
    "    \"\"\"\n",
    "    Extracts topics and their associated words from a PDF document using the \n",
    "    Latent Dirichlet Allocation (LDA) algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): The path to the PDF file for topic extraction.\n",
    "        num_topics (int): The number of topics to discover.\n",
    "        words_per_topic (int): The number of words to include per topic.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of num_topics sublists, each containing relevant words \n",
    "        for a topic.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file, on_bad_lines='skip', encoding='latin1')\n",
    "\n",
    "    # Check if 'text' column exists\n",
    "    if 'conclusion' not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain a 'text' column\")\n",
    "\n",
    "    # Extract the text into a list. Each row is considered a document\n",
    "    documents = df['conclusion'].tolist()\n",
    "\n",
    "    # Preprocess the documents\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words(['english','spanish']))\n",
    "    processed_documents = [preprocess(doc, stop_words) for doc in documents]\n",
    "\n",
    "    # Create a dictionary and a corpus\n",
    "    dictionary = corpora.Dictionary(processed_documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "\n",
    "    # Build the LDA model\n",
    "    lda_model = LdaModel(\n",
    "        corpus, \n",
    "        num_topics=num_topics, \n",
    "        id2word=dictionary, \n",
    "        passes=15\n",
    "        )\n",
    "\n",
    "    # Retrieve the topics and their corresponding words\n",
    "    topics = lda_model.print_topics(num_words=words_per_topic)\n",
    "\n",
    "    # Store each list of words from each topic into a list\n",
    "    topics_ls = []\n",
    "    for topic in topics:\n",
    "        words = topic[1].split(\"+\")\n",
    "        topic_words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
    "        topics_ls.append(topic_words)\n",
    "\n",
    "    return topics_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_from_corpus(llm, file, num_topics, words_per_topic):\n",
    "    \"\"\"\n",
    "    Generates descriptive prompts for LLM based on topic words extracted from a \n",
    "    PDF document.\n",
    "\n",
    "    This function takes the output of `get_topic_lists_from_pdf` function, \n",
    "    which consists of a list of topic-related words for each topic, and \n",
    "    generates an output string in table of content format.\n",
    "\n",
    "    Parameters:\n",
    "        llm (LLM): An instance of the Large Language Model (LLM) for generating \n",
    "        responses.\n",
    "        file (str): The path to the PDF file for extracting topic-related words.\n",
    "        num_topics (int): The number of topics to consider.\n",
    "        words_per_topic (int): The number of words per topic to include.\n",
    "\n",
    "    Returns:\n",
    "        str: A response generated by the language model based on the provided \n",
    "        topic words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract topics and convert to string\n",
    "    list_of_topicwords = get_topic_lists_from_corpus(file, num_topics, \n",
    "                                                  words_per_topic)\n",
    "    string_lda = \"\"\n",
    "    for topic_words in list_of_topicwords:\n",
    "        string_lda += str(topic_words) + \"\\n\"\n",
    "\n",
    "    # Create the template\n",
    "    template_string = '''Describe the topic of each of the {num_topics} and give the topic a name\n",
    "        double-quote delimited lists in a simple sentence and provide a set of relevant keywords.\n",
    "        The lists are the result of an algorithm for topic discovery.\n",
    "        Do not provide an introduction or a conclusion, only describe the\n",
    "        topics. Do not mention the word \"topic\" when describing the topics.\n",
    "        Use the following template for the response.\n",
    "\n",
    "        1: <<<(Topic Name)(sentence describing the topic)>>>\n",
    "        - Keywords: <<<(Comma-separated keywords)>>>\n",
    "\n",
    "        2: <<<(Topic Name)(sentence describing the topic)>>>\n",
    "        - Keywords: <<<(Comma-separated keywords)>>>\n",
    "\n",
    "        ...\n",
    "\n",
    "        n: <<<(Topic Name)(sentence describing the topic)>>>\n",
    "        - Keywords: <<<(Comma-separated keywords)>>>\n",
    "\n",
    "        Lists: \"\"\"{string_lda}\"\"\" '''\n",
    "\n",
    "    # LLM call\n",
    "    prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run({\n",
    "        \"string_lda\": string_lda,\n",
    "        \"num_topics\": num_topics\n",
    "    })\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = \"\"\n",
    "llm = OpenAI(openai_api_key=openai_key, max_tokens=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yijin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "file = \"Report Dataset.csv\"\n",
    "num_topics = 15\n",
    "words_per_topic = 15\n",
    "\n",
    "summary = topics_from_corpus(llm, file, num_topics, words_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1: (Non-convulsive Seizures) This topic discusses the physical manifestation of non-convulsive seizures, including their status, periodic features, and induction through midazolam.\n",
      "- Keywords: non-convulsive, seizures, status, periodic, midazolam, induction, burst, suppression, stimulation, features\n",
      "\n",
      "2: (Abnormal Brain Activity) This topic focuses on abnormal brain activity and its association with changes, events, and activities, as well as its comparison to normal brain activity.\n",
      "- Keywords: abnormal, brain activity, changes, events, activities, comparison, beta, mildly, excessive \n",
      "\n",
      "3: (Epileptiform Discharges) This topic explores the presence of epileptiform discharges in EEG recordings, including their features, improvement, and focal characteristics.\n",
      "- Keywords: epileptiform, discharges, EEG recordings, features, improvement, focal, background, findings \n",
      "\n",
      "4: (Underlying Brain Abnormality) This topic discusses the abnormal brain activity seen in EEG recordings, specifically slow wave activity and its association with underlying structural abnormalities.\n",
      "- Keywords: abnormal brain activity, EEG, slow waves, underlying, abnormality, structural, slow, alpha, theta, delta \n",
      "\n",
      "5: (Epilepsy and EEG) This topic looks at the correlation between EEG findings and clinical epilepsy, as well as the presence of ongoing discharges during sleep and their indication of seizure liability.\n",
      "- Keywords: epilepsy, EEG, correlation, clinical, sleep, discharges, ongoing, liability, abnormal \n",
      "\n",
      "6: (Frontal Lobe EEG Activity) This topic examines the EEG findings in the frontal region during sleep, including the presence of sharp waves and their correlation with imaging results.\n",
      "- Keywords: frontal lobe, EEG, sleep, sharp waves, imaging, frequency, frontal region, imaging, findings \n",
      "\n",
      "7: (Diffuse Cortical Dysfunction) This topic delves into the EEG findings indicative of diffuse cortical dysfunction, including slow wave activity, irregular Hz, and moderate to severe abnormalities.\n",
      "- Keywords: diffuse, cortical dysfunction, EEG, slow waves, irregular, Hz, moderate, severe, encephalopathy \n",
      "\n",
      "8: (Generalized Epilepsy) This topic focuses on the EEG findings indicative of generalized epilepsy, such as generalized spike-wave discharges and polyspikes during sleep.\n",
      "- Keywords: generalized, epilepsy, EEG, discharges, spike-wave, sleep, polyspikes, diagnosis \n",
      "\n",
      "9: (Temporal Lobe Epilepsy) This topic discusses the EEG findings in patients with temporal lobe epilepsy, including frequent sharp wave activity in the left and right regions.\n",
      "- Keywords: temporal lobe, EEG, epilepsy, sharp waves, frequent, left, right, focal \n",
      "\n",
      "10: (Normal EEG in Children) This topic examines the normal EEG findings in children, both awake and during sleep, and how they differ from adults.\n",
      "- Keywords: normal, EEG, children, awake, sleep, age, bisynchronous, test \n",
      "\n",
      "11: (Focal EEG Findings) This topic looks at the EEG findings in patients with focal epilepsy, including the presence of slow discharges in specific regions and their correlation with neuroimaging.\n",
      "- Keywords: focal, EEG, epilepsy, slow discharges, focal regions, neuroimaging, suggestive \n",
      "\n",
      "12: (Focal Seizures) This topic discusses the EEG findings in patients with focal seizures, including sharp waves and slow activity in the region of the head where the seizure originates.\n",
      "- Keywords: focal, seizures, EEG, sharp waves, slow activity, region of origin \n",
      "\n",
      "13: (Cortical Dysfunction in EEG) This topic explores the EEG findings indicative of cortical dysfunction, including intermittent slowing, polymorphic activity, and mild to moderate abnormalities.\n",
      "- Keywords: cortical dysfunction, EEG, intermittent slowing, polymorphic, mild, moderate, suggestive \n",
      "\n",
      "14: (Abnormal Brain Activity in EEG) This topic discusses the EEG findings indicative of abnormal brain activity, including slow waves, delta activity, and frequent sharp waves.\n",
      "- Keywords: abnormal, brain activity, EEG, slow waves, delta, sharp waves, frequency \n",
      "\n",
      "15: (Hemispheric EEG Asymmetry) This topic examines the EEG findings of asymmetry between the hemispheres, specifically in response to stimuli and sensory activity.\n",
      "- Keywords: hemispheric, EEG, asymmetry, response, stimuli, sensory, attenuated, pleads, marked, reactive\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesistf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
